[
  {
    "objectID": "posts/VODS/index.html",
    "href": "posts/VODS/index.html",
    "title": "1er Premio en el Concurso de Visualización de Datos de la ciudad de Valencia 2023",
    "section": "",
    "text": "Primer Premio obtenido en el Concurso de Visualización de Datos de la ciudad de Valencia 2023 organizado por la Universitat de Valencia en colaboración con el Ayuntamiento de Valencia.\n\n\n\n\n Volver arriba"
  },
  {
    "objectID": "posts/Pynq Z2/index.html",
    "href": "posts/Pynq Z2/index.html",
    "title": "Estación MedioAmbiental Inteligente",
    "section": "",
    "text": "Desarrollo de una Estación MedioAmbiental Inteligente como proyecto IoT (Internet de las cosas) mediante el uso de la placa PYNQ Z2 y el Arduino Shield.\n\n\n\n\n Volver arriba"
  },
  {
    "objectID": "posts/Fases del Suenyo/index.html",
    "href": "posts/Fases del Suenyo/index.html",
    "title": "Detección de las Fases del Sueño mediante XGBoost.",
    "section": "",
    "text": "El sueño es un proceso biológico compuesto por una serie de fases que permiten la restauración física y mental del cuerpo. Estas fases, que incluyen el sueño ligero (N1 y N2), el sueño profundo (N3) y el sueño REM. La comprensión de estas etapas resulta esencial en el ámbito de la analítica de datos en salud, particularmente en el diagnóstico y tratamiento de trastornos del sueño.\nEn este trabajo se aborda el análisis de datos polisomnográficos (PSG) utilizando el conjunto de datos ISRUC-Sleep, que contiene en su tercer subgrupo las fases del sueño recogidas en 10 pacientes. En el proyecto se aplicarán distintos modelos XGBoost para automatizar la detección de las distintas fases del sueño a través de la extracción de características de los archivos .edf."
  },
  {
    "objectID": "posts/Fases del Suenyo/index.html#información-sobre-la-base-de-datos",
    "href": "posts/Fases del Suenyo/index.html#información-sobre-la-base-de-datos",
    "title": "Detección de las Fases del Sueño mediante XGBoost.",
    "section": "3.1 Información sobre la base de datos",
    "text": "3.1 Información sobre la base de datos\nLee: http://dataset.isr.uc.pt/ISRUC_Sleep/Content.pdf\nCada archivo comprimido contiene:\n- Un archivo .rec, que en realidad es un archivo .edf (RENÓMBRALO de .rec a .edf).\n- Dos archivos .txt que son las anotaciones de los especialistas.\n- Dos archivos .xlsx que contienen la misma información (Etapa) y más datos (muy útiles si deseas entender por qué se cometen errores, descartar épocas de calidad cuestionable, etc.).\nEl PSG está compuesto por señales de los siguientes canales:\n\nEEG (F3, C3, O1, F4, C4 y O2)\n\nElectrooculograma (EOG), derecho e izquierdo (ROC y LOC)\n\nElectrocardiograma (ECG)\n\nTipos de EMG: un m. submentalis (EMG del mentón, X1) y dos m. tibialis (EMGs de las piernas)\n\nLas referencias se colocaron en los lóbulos de las orejas izquierda y derecha (A1, A2)."
  },
  {
    "objectID": "posts/Fases del Suenyo/index.html#carga-de-los-datos",
    "href": "posts/Fases del Suenyo/index.html#carga-de-los-datos",
    "title": "Detección de las Fases del Sueño mediante XGBoost.",
    "section": "3.2 Carga de los datos",
    "text": "3.2 Carga de los datos\n\ndef load_raw(test=False):\n\n    raw_list = []\n    if test:\n        n1 = 9\n        n2 = 10\n    else:\n        n1 = 1\n        n2 = 8\n\n    for n in range(n1,n2+1):\n        path = \"data/\" + str(n) + \"/\" + str(n) + \".edf\"\n\n        raw = mne.io.read_raw_edf(path, preload=True, verbose=False)\n\n        # Change the freq. to 100 Hz to reduce compute time (we don't have any frequencies of interest above 50Hz)\n        raw.resample(100)\n\n        # Filter to remove base line (very low freqs. produced by breathing, leg movement, etc.) \n        raw.filter(0.3, 49, verbose=False)\n\n        # Append to the list\n        raw_list.append(raw)\n    \n    return raw_list\n\ndef observe_data(raw_list: list):\n\n    fig, axes = plt.subplots(len(raw_list), 1, sharex=True, sharey=False, figsize = (len(raw_list),len(raw_list)*2))\n    cont = 1\n    for (ax, raw) in zip(axes, raw_list):\n\n        # Observe the data\n        sf = raw.info['sfreq']\n        chan = raw.ch_names\n\n        # Warning: change the scale of the data if we access it directly!\n        data = raw.get_data() * 1e6 #m icroVolts (mne works in V)\n        data = data[:,:-int(30*sf)] # Delete te last 30s because the reference article does so\n\n        # Plot the signal\n        times = np.arange(data.shape[-1]) / sf\n        time_range= slice(int(50*sf), int(100*sf)) # Choose some seconds range to plot\n        ax.plot(times[time_range], data[0, time_range], lw=1.5, color='k')\n        ax.set_title('Patient ' + str(cont))\n        cont += 1\n    \n    fig.suptitle(\"Datos del sueño EEG\")\n    fig.supxlabel(\"Tiempo (segundos)\")\n    fig.supylabel(\"Amplitud (V)\")\n    plt.show()\n    return\n\nA continuación se cargan los primeros 8 pacientes, los cuales serán utilizados para el conjunto de entrenamiento y validación.\n\nraw_train_list = load_raw()\n\n\n3.2.1 Etiquetas de Fases del Sueño\nA continuación, se detalla la codificación de las fases en los archivos del conjunto de datos ISRUC-SLEEP.\nTen cuidado, ya que no existe el valor 4 (anteriormente se distinguía una fase adicional).\n\n0 -&gt; W (Vigilia)\n\n1 -&gt; N1\n\n2 -&gt; N2\n\n3 -&gt; N3\n\n5 -&gt; REM\n\nEl formato predeterminado del hipnograma en YASA es un vector entero unidimensional donde:\n\n-2 = Sin puntuación\n\n-1 = Artefacto / Movimiento\n\n0 = Vigilia (Wake)\n\n1 = Sueño N1\n\n2 = Sueño N2\n\n3 = Sueño N3\n\n4 = Sueño REM\n\n\ndef load_labels(test=False):\n\n    labels_list = []\n    if test:\n        n1 = 9\n        n2 = 10\n    else:\n        n1 = 1\n        n2 = 8\n\n    for n in range(n1,n2+1):\n        path = \"data/\" + str(n) + \"/\" + str(n) + \"_1.txt\"\n\n        labels = pd.read_csv(path, header=None).squeeze(\"columns\")\n\n        labels[labels == 5] = 4  # Recode REM from 5 to 4 for YASA compatibility\n        labels = labels[:-30]  # Remove last 30 epochs (each epoch is 30s)\n\n        # Append to the list\n        labels_list.append(labels)\n    \n    return labels_list\n\ndef observe_labels(labels_list):\n\n    # Create a mapping for sleep stages\n    sleep_stages = {\n        0: 'Wake',\n        1: 'N1 sleep',\n        2: 'N2 sleep',\n        3: 'N3 sleep',\n        4: 'REM sleep'\n    }\n\n    fig, axes = plt.subplots(int(len(labels_list) / 2), 2, sharex=True, sharey=False, figsize = (len(labels_list)+5,len(labels_list)))\n    contx = 0\n    conty = 0\n    cont = 1\n\n    for labels in labels_list:\n\n        # Calculate accumulated time in each phase (in minutes)\n        accumulated_time = labels.value_counts() * 0.5  # Each sample is 30 seconds, so multiply by 0.5 to get minutes\n\n        # Plot the distribution of sleep stages\n        sns.barplot(x = accumulated_time.index.map(sleep_stages), y=accumulated_time.values, order=sleep_stages.values(), ax=axes[conty,contx])\n        axes[conty, contx].set_ylabel(\"\")\n        axes[conty, contx].set_xlabel(\"\")\n        axes[conty, contx].set_title('Paciente ' + str(cont))\n        conty += 1\n        cont += 1\n\n        if conty == 4:\n            conty = 0\n            contx = 1\n    \n    fig.suptitle(\"Tiempo Acumulado por Etapa del Sueño\")\n    fig.supxlabel(\"Estados del Sueño\")\n    fig.supylabel(\"Tiempo (minutos)\")\n    plt.show()\n    return\n\ndef observe_hypnogram(labels_list):\n\n    fig, axes = plt.subplots(int(len(labels_list) / 2), 2, sharex=True, sharey=False, figsize = (len(labels_list)+5,len(labels_list)))\n    contx = 0\n    conty = 0\n    cont = 1\n\n    for labels in labels_list:\n        yasa.plot_hypnogram(labels, ax=axes[conty,contx])\n        axes[conty, contx].set_ylabel(\"\")\n        axes[conty, contx].set_xlabel(\"\")\n        axes[conty, contx].set_title('Paciente ' + str(cont))\n        conty += 1\n        cont += 1\n        \n        if conty == 4:\n            conty = 0\n            contx = 1\n    \n    fig.suptitle(\"Hipnograma\")\n    fig.supxlabel(\"Tiempo (horas)\")\n    fig.supylabel(\"Estado\")\n    plt.show()\n    return\n\nlabels_train_list = load_labels()\n\nobserve_labels(labels_train_list)\n\n\n\n\n\n\n\n\n\nobserve_hypnogram(labels_train_list)\n\nC:\\Users\\enriq\\AppData\\Local\\miniconda3\\envs\\ads\\Lib\\site-packages\\yasa\\plotting.py:89: FutureWarning: 'S' is deprecated and will be removed in a future version. Please use 's' instead of 'S'.\n  freq_str = pd.tseries.frequencies.to_offset(pd.Timedelta(1 / sf_hypno, \"S\")).freqstr\nC:\\Users\\enriq\\AppData\\Local\\miniconda3\\envs\\ads\\Lib\\site-packages\\yasa\\plotting.py:89: FutureWarning: 'S' is deprecated and will be removed in a future version. Please use 's' instead of 'S'.\n  freq_str = pd.tseries.frequencies.to_offset(pd.Timedelta(1 / sf_hypno, \"S\")).freqstr\nC:\\Users\\enriq\\AppData\\Local\\miniconda3\\envs\\ads\\Lib\\site-packages\\yasa\\plotting.py:89: FutureWarning: 'S' is deprecated and will be removed in a future version. Please use 's' instead of 'S'.\n  freq_str = pd.tseries.frequencies.to_offset(pd.Timedelta(1 / sf_hypno, \"S\")).freqstr\nC:\\Users\\enriq\\AppData\\Local\\miniconda3\\envs\\ads\\Lib\\site-packages\\yasa\\plotting.py:89: FutureWarning: 'S' is deprecated and will be removed in a future version. Please use 's' instead of 'S'.\n  freq_str = pd.tseries.frequencies.to_offset(pd.Timedelta(1 / sf_hypno, \"S\")).freqstr\nC:\\Users\\enriq\\AppData\\Local\\miniconda3\\envs\\ads\\Lib\\site-packages\\yasa\\plotting.py:89: FutureWarning: 'S' is deprecated and will be removed in a future version. Please use 's' instead of 'S'.\n  freq_str = pd.tseries.frequencies.to_offset(pd.Timedelta(1 / sf_hypno, \"S\")).freqstr\nC:\\Users\\enriq\\AppData\\Local\\miniconda3\\envs\\ads\\Lib\\site-packages\\yasa\\plotting.py:89: FutureWarning: 'S' is deprecated and will be removed in a future version. Please use 's' instead of 'S'.\n  freq_str = pd.tseries.frequencies.to_offset(pd.Timedelta(1 / sf_hypno, \"S\")).freqstr\nC:\\Users\\enriq\\AppData\\Local\\miniconda3\\envs\\ads\\Lib\\site-packages\\yasa\\plotting.py:89: FutureWarning: 'S' is deprecated and will be removed in a future version. Please use 's' instead of 'S'.\n  freq_str = pd.tseries.frequencies.to_offset(pd.Timedelta(1 / sf_hypno, \"S\")).freqstr\nC:\\Users\\enriq\\AppData\\Local\\miniconda3\\envs\\ads\\Lib\\site-packages\\yasa\\plotting.py:89: FutureWarning: 'S' is deprecated and will be removed in a future version. Please use 's' instead of 'S'.\n  freq_str = pd.tseries.frequencies.to_offset(pd.Timedelta(1 / sf_hypno, \"S\")).freqstr"
  },
  {
    "objectID": "posts/Fases del Suenyo/index.html#biblioteca-yasa",
    "href": "posts/Fases del Suenyo/index.html#biblioteca-yasa",
    "title": "Detección de las Fases del Sueño mediante XGBoost.",
    "section": "4.1 Biblioteca yasa",
    "text": "4.1 Biblioteca yasa\nYASA (Yet Another Spindle Algorithm) es una herramienta en Python diseñada para analizar datos de polisomnografía (PSG) y clasificar etapas del sueño.\nPrincipales funcionalidades:\n\nClasificación de etapas del sueño: Automáticamente identifica N1, N2, N3 y REM.\n\n\nDetección de eventos: Husos, ondas lentas y movimientos oculares rápidos.\n\n\nRechazo de artefactos: Limpia señales EEG para mejorar la calidad del análisis.\n\n\nAnálisis espectral: Potencia por bandas, pendiente \\(1/f\\), acoplamiento fase-amplitud, etc.\n\n\nHipnogramas: Genera gráficos y estadísticas sobre las etapas del sueño.\n\nSe trata de una buena herramienta, práctica y enfocada en la investigación del sueño.\nCabe destacar que YASA elimina los picos de sueño, por eso no es necesario utilizar los datos normalizados y en su lugar se hace uso de los originales."
  },
  {
    "objectID": "posts/Fases del Suenyo/index.html#características-extraídas",
    "href": "posts/Fases del Suenyo/index.html#características-extraídas",
    "title": "Detección de las Fases del Sueño mediante XGBoost.",
    "section": "4.2 Características extraídas",
    "text": "4.2 Características extraídas\nPara cada época de 30 segundos y cada canal, se calculan las siguientes características:\n\nabspow (Potencia absoluta)\n\nEs la energía total de la señal en un rango de frecuencias específico (por ejemplo, delta, theta, alfa, beta, etc.).\n\nEn EEG: Refleja la amplitud promedio de las oscilaciones cerebrales en un rango determinado y se mide en microvoltios al cuadrado.\n\n\nEn EMG: Un aumento indica mayor actividad muscular (movimientos o contracciones).\n\n\nEn EOG: Un aumento puede indicar movimientos oculares intensos (parpadeos o REM).\n\n\nalpha (Potencia en el rango alfa)\n\n\nEn EEG: Mide la potencia de la señal EEG en el rango alfa (8-13 Hz), comúnmente asociada con estados de relajación y calma, especialmente durante la transición al sueño o relajación con los ojos cerrados.\n\n\nEn EMG: El rango alfa no tiene una correlación directa con relajación (como en EEG). Sin embargo, puede reflejar actividad muscular en frecuencias bajas, especialmente durante movimientos lentos o tensiones isométricas.\n\n\nEn EOG: El rango alfa puede reflejar actividad asociada con movimientos lentos del ojo durante la transición al sueño.\n\n\nat (Relación alfa-theta)\n\nCociente entre la potencia de las ondas alfa y theta.\n\nEn EEG: Este índice puede usarse para determinar el grado de relajación o alerta.\n\n\nEn EMG: Se puede usar para comparar actividad de baja frecuencia entre músculos en reposo versus en movimiento.\n\n\nEn EOG: Indica la relación entre movimientos oculares lentos y más rápidos. Puede usarse para diferenciar estados de sueño (por ejemplo, transición al REM).\n\n\nbeta (Potencia en el rango beta)\n\nRepresenta la potencia en el rango beta (13-30 Hz).\n\nEn EEG: Estas ondas están asociadas con actividad mental, concentración y, en algunos casos, estrés o ansiedad.\n\n\nEn EMG: Representa actividad muscular en frecuencias más altas, asociada con movimientos musculares rápidos o contracciones voluntarias.\n\n\nEn EOG: Potencia en frecuencias más altas, asociada con movimientos oculares rápidos o actividad relacionada con parpadeos.\n\n\ndb (Relación delta-beta)\n\nCociente entre las potencias en el rango delta (0.5-4 Hz) y beta (13-30 Hz).\n\nEn EEG: Un aumento en la relación puede indicar somnolencia o disminución de la actividad cortical.\n\n\nEn EMG: Una mayor relación delta-beta podría sugerir períodos de descanso muscular, ya que las frecuencias más bajas (delta) dominan en músculos relajados.\n\n\nEn EOG: Mayor relación indica predominancia de movimientos lentos del ojo, como en etapas tempranas del sueño.\n\n\nds (Pendiente espectral delta)\n\n\nEn EEG: Analiza la pendiente de la potencia en el rango delta en el dominio de frecuencias, lo que puede reflejar cambios en la profundidad del sueño.\n\n\nEn EMG: Analiza cómo decae la potencia en el rango delta de la señal muscular. Una pendiente más pronunciada puede indicar menor actividad muscular de baja frecuencia.\n\n\nEn EOG: Evalúa la pendiente en el rango delta, reflejando la intensidad de movimientos lentos del ojo.\n\n\ndt (Relación delta-theta)\n\nCociente entre las potencias en los rangos delta (0.5-4 Hz) y theta (4-8 Hz).\n\nEn EEG: Se utiliza para estudiar la arquitectura del sueño, ya que estas frecuencias predominan en las etapas profundas y ligeras del sueño, respectivamente.\n\n\nEn EMG: Relación entre las frecuencias bajas en reposo y actividad muscular ligera. Es útil para monitorear relajación versus activación.\n\n\nEn EOG: Cociente entre movimientos oculares lentos y más rápidos, útil para analizar transiciones entre sueño ligero y REM.\n\n\nfdelta (Frecuencia dominante en delta)\n\nRepresenta la frecuencia dentro del rango delta con mayor potencia.\n\nEn EEG: Es un marcador de sueño profundo (NREM etapa 3).\n\n\nEn EMG: Indica la frecuencia más representativa en el rango delta de la señal muscular. Un valor alto puede reflejar vibraciones musculares lentas.\n\n\nEn EOG: Frecuencia más representativa en movimientos lentos del ojo. Una frecuencia alta puede reflejar parpadeos frecuentes.\n\n\nhcomp (Complejidad de Higuchi)\n\nEstima la complejidad de la señal EEG utilizando el algoritmo de Higuchi.\n\nEn EEG: Es útil para analizar fenómenos no lineales en las señales EEG, como la transición entre vigilia y sueño.\n\n\nEn EMG: Mide la complejidad de la señal muscular. Movimientos complejos o transitorios (como espasmos) aumentan esta medida.\n\n\nEn EOG: Refleja la complejidad de los movimientos oculares, como transiciones rápidas durante REM.\n\n\nhiguchi (Dimensión fractal de Higuchi)\n\nOtra medida de la complejidad basada en la geometría fractal de la señal EEG.\n\nEn EEG: Valores más altos indican mayor desorganización o actividad caótica.\n\n\nEn EMG: Evalúa la estructura fractal de la señal. Es útil para detectar patrones irregulares en la actividad muscular.\n\n\nEn EOG: Detecta patrones irregulares en movimientos oculares.\n\n\nhmob (Movilidad de Hjorth)\n\nMide la tasa de cambio de la amplitud de la señal, reflejando la velocidad de cambio en el EEG.\n\nEn EEG: Está relacionado con la actividad cortical.\n\n\nEn EMG: Refleja la rapidez con la que cambia la señal de EMG, asociada con contracciones musculares rápidas o sostenidas.\n\n\nEn EOG: Mide la rapidez de cambio en los movimientos oculares.\n\n\niqr (Rango intercuartílico)\n\nMide la dispersión de la amplitud de la señal entre el percentil 25 y el 75.\n\nEn EEG: Ayuda a eliminar el efecto de valores atípicos en el análisis de amplitud.\n\n\nEn EMG: Analiza la variación central de la señal de EMG, eliminando valores extremos que podrían ser artefactos o picos.\n\n\nEn EOG: Variabilidad en la amplitud de la señal ocular, eliminando valores extremos (parpadeos).\n\n\nkurt (Curtosis)\n\nMide la forma de la distribución de la amplitud de la señal.\n\nEn EEG: Una curtosis elevada indica la presencia de picos o eventos raros de alta amplitud.\n\n\nEn EMG: Identifica eventos extremos en la señal, como movimientos bruscos o contracciones repentinas.\n\n\nEn EOG: Identifica movimientos oculares extremos, como REM intenso.\n\n\nnzc (Cruces por cero)\n\nCuenta el número de veces que la señal cruza el eje cero en un intervalo.\n\nEn EEG: Se relaciona con la frecuencia dominante de la señal.\n\n\nEn EMG: Refleja la frecuencia de oscilaciones musculares, útil para analizar patrones repetitivos como temblores.\n\n\nEn EOG: Refleja la frecuencia de oscilaciones en los movimientos oculares. Útil para identificar patrones como REM.\n\n\nperm (Entropía de permutación)\n\nMide la complejidad de la señal basada en la probabilidad de ocurrencia de patrones específicos en el tiempo.\n\nEn EEG: Se usa para caracterizar la desorganización de las señales EEG.\n\n\nEn EMG: Mide la aleatoriedad de la señal muscular. Una mayor entropía indica actividad más desorganizada, como en espasmos o movimientos bruscos.\n\n\nEn EOG: Mide la aleatoriedad de los movimientos oculares.\n\n\npetrosian (Dimensión fractal de Petrosian)\n\nUna medida de la complejidad de la señal basada en cambios abruptos en la dirección del EEG.\n\nEn EEG: Útil para detectar eventos episódicos o transitorios.\n\n\nEn EMG: Detecta cambios abruptos en la señal, característicos de movimientos rápidos o actividad muscular transitoria.\n\n\nEn EOG: Detecta cambios abruptos en la dirección de los movimientos oculares.\n\n\nsdelta (Potencia relativa delta)\n\nRepresenta la proporción de la potencia en el rango delta respecto a la potencia total de la señal.\n\nEn EEG: Se usa para evaluar la calidad del sueño profundo.\n\n\nEn EMG: Puede reflejar relajación muscular.\n\n\nEn EOG: Proporción de movimientos oculares lentos respecto al total.\n\n\nsigma (Potencia en el rango sigma)\n\nPotencia en el rango sigma (12-16 Hz), a menudo asociado con los husos del sueño (spindles).\n\nEn EEG: Los husos son importantes para consolidar la memoria durante el sueño.\n\n\nEn EMG: Asociada con actividad muscular moderada, como movimientos rítmicos.\n\n\nEn EOG: Asociada con movimientos rítmicos o transitorios en frecuencias medias.\n\n\nskew (Asimetría o sesgo)\n\nIndica el sesgo en la distribución de la amplitud de la señal.\n\nEn EEG: Valores positivos o negativos indican si la señal está más cargada hacia valores altos o bajos.\n\n\nEn EMG: Una asimetría elevada puede reflejar contracciones musculares irregulares.\n\n\nEn EOG: Refleja la dirección predominante de los movimientos oculares.\n\n\nstd (Desviación estándar)\n\nRefleja la variabilidad de la amplitud de la señal en un intervalo.\n\nEn EEG: Valores más altos indican mayor amplitud promedio.\n\n\nEn EMG: Una alta desviación indica contracciones intensas.\n\n\nEn EOG: Variabilidad en la amplitud de los movimientos oculares.\n\n\ntheta (Potencia en el rango theta)\n\nMide la potencia en el rango theta (4-8 Hz).\n\nEn EEG: Asociada con la transición al sueño ligero y actividades relacionadas con el descanso y la relajación.\n\n\nEn EMG: Actividad muscular en el rango theta, que puede aparecer en movimientos lentos o relajación muscular.\n\n\nEn EOG: Actividad ocular lenta, típica de las primeras etapas del sueño.\n\nAdemás, el algoritmo calcula versiones suavizadas y normalizadas de estas características. Específicamente, se aplica un promedio móvil triangular ponderado centrado de 7.5 minutos y un promedio móvil de 2 minutos hacia atrás. Las características suavizadas resultantes se normalizan utilizando un z-score robusto."
  },
  {
    "objectID": "posts/Fases del Suenyo/index.html#extracción-de-características-1",
    "href": "posts/Fases del Suenyo/index.html#extracción-de-características-1",
    "title": "Detección de las Fases del Sueño mediante XGBoost.",
    "section": "4.3 Extracción de características",
    "text": "4.3 Extracción de características\n\ndef load_features(raw_list, labels_list):\n    \n    sls_pd = pd.DataFrame()\n\n    paciente = 1\n\n    for (raw, labels) in zip(raw_list, labels_list):\n\n        # Obtain features using SleepStaging\n        sls = yasa.SleepStaging(raw, eeg_name ='C4-A1', eog_name='LOC-A2', emg_name='X1')\n\n        # Eliminamos las 30 epochs (each 30s)\n        sls2 = sls.get_features()[:-30]\n        sls2[\"paciente\"] = paciente\n        sls2[\"y\"] = labels\n\n        sls_pd = pd.concat([sls_pd,sls2])\n\n        paciente += 1\n    \n    return sls_pd\n\nsls_pd = load_features(raw_train_list, labels_train_list)\n\n\nsls_pd.describe()\n\n\n\n\n\n\n\n\neeg_abspow\neeg_abspow_c7min_norm\neeg_abspow_p2min_norm\neeg_alpha\neeg_alpha_c7min_norm\neeg_alpha_p2min_norm\neeg_at\neeg_at_c7min_norm\neeg_at_p2min_norm\neeg_beta\n...\neog_std\neog_std_c7min_norm\neog_std_p2min_norm\neog_theta\neog_theta_c7min_norm\neog_theta_p2min_norm\ntime_hour\ntime_norm\npaciente\ny\n\n\n\n\ncount\n6884.000000\n6884.000000\n6884.000000\n6884.000000\n6884.000000\n6884.000000\n6884.000000\n6884.000000\n6884.000000\n6884.000000\n...\n6884.000000\n6884.000000\n6884.000000\n6884.000000\n6884.000000\n6884.000000\n6884.000000\n6884.000000\n6884.000000\n6884.000000\n\n\nmean\n2.392229\n0.116906\n0.118064\n0.146083\n0.064154\n0.072592\n1.251271\n0.105367\n0.108628\n0.053647\n...\n1.400236\n0.063859\n0.081206\n0.126483\n-0.015880\n-0.018094\n3.606930\n0.483140\n4.494480\n1.961360\n\n\nstd\n2.338407\n0.323609\n0.328050\n0.157908\n0.312899\n0.312902\n2.681659\n0.324024\n0.327019\n0.058473\n...\n0.833542\n0.319383\n0.333004\n0.061784\n0.323672\n0.325173\n2.114175\n0.279290\n2.345093\n1.302203\n\n\nmin\n0.331484\n-0.333267\n-0.305903\n0.007839\n-0.556872\n-0.557903\n0.123109\n-0.493018\n-0.471918\n0.000868\n...\n0.428800\n-0.449693\n-0.485156\n0.007737\n-0.803942\n-0.805737\n0.000000\n0.000000\n1.000000\n0.000000\n\n\n25%\n0.994142\n-0.110449\n-0.112493\n0.067778\n-0.099404\n-0.096087\n0.416017\n-0.071396\n-0.062917\n0.016269\n...\n0.904066\n-0.169171\n-0.148022\n0.076466\n-0.279938\n-0.296911\n1.791667\n0.241318\n2.000000\n1.000000\n\n\n50%\n1.645745\n0.001986\n0.002210\n0.101435\n-0.001646\n-0.001109\n0.592628\n-0.000695\n-0.000438\n0.035328\n...\n1.179361\n-0.000800\n0.000013\n0.123724\n0.002765\n0.001601\n3.583333\n0.483187\n5.000000\n2.000000\n\n\n75%\n2.967875\n0.249257\n0.245971\n0.148511\n0.162140\n0.169099\n0.874097\n0.156698\n0.151154\n0.066407\n...\n1.615627\n0.236022\n0.223445\n0.167932\n0.236333\n0.233239\n5.375000\n0.725083\n7.000000\n3.000000\n\n\nmax\n36.887089\n1.469028\n2.032152\n0.902945\n1.312294\n1.621920\n31.961843\n1.825480\n2.550393\n0.574367\n...\n11.639682\n1.726705\n2.496414\n0.485273\n0.773472\n1.151591\n8.075000\n0.969970\n8.000000\n4.000000\n\n\n\n\n8 rows × 151 columns\n\n\n\n\n4.3.1 Análisis Exploratorio\n\nSeleccionamos las variables no normalizadas\n\n\nsls_pd = sls_pd.loc[:, ~sls_pd.columns.str.contains(\"norm\")]\nsls_pd\n\n\n\n\n\n\n\n\neeg_abspow\neeg_alpha\neeg_at\neeg_beta\neeg_db\neeg_ds\neeg_dt\neeg_fdelta\neeg_hcomp\neeg_higuchi\n...\neog_perm\neog_petrosian\neog_sdelta\neog_sigma\neog_skew\neog_std\neog_theta\ntime_hour\npaciente\ny\n\n\nepoch\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n2.223068\n0.306459\n4.183846\n0.126726\n3.420603\n8.966586\n5.917954\n0.240573\n2.320056\n1.823471\n...\n0.932268\n1.020032\n0.379949\n0.017829\n-0.255218\n3.658173\n0.027003\n0.000000\n1\n0\n\n\n1\n1.739309\n0.214499\n1.858936\n0.142990\n3.273220\n9.813575\n4.056213\n0.301374\n2.060777\n1.813796\n...\n0.930587\n1.019913\n0.294492\n0.036311\n-0.012890\n2.873480\n0.048089\n0.008333\n1\n0\n\n\n2\n10.646273\n0.053756\n1.102060\n0.103112\n7.565166\n23.103180\n15.992047\n0.138216\n2.405262\n1.746981\n...\n0.943526\n1.020977\n0.443489\n0.052026\n0.080295\n3.556994\n0.042671\n0.016667\n1\n0\n\n\n3\n3.218050\n0.125759\n1.339224\n0.195343\n2.857057\n8.191535\n5.943351\n0.364850\n2.384103\n1.798444\n...\n0.949502\n1.021550\n0.403663\n0.043639\n-0.410182\n2.967387\n0.048401\n0.025000\n1\n0\n\n\n4\n1.839396\n0.145482\n1.453737\n0.268479\n1.486379\n4.766511\n3.987657\n0.157197\n2.086368\n1.840072\n...\n0.951833\n1.021755\n0.187960\n0.063337\n0.131732\n2.672432\n0.039588\n0.033333\n1\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n965\n1.065989\n0.146153\n0.608933\n0.037588\n14.911792\n14.893099\n2.335285\n0.440802\n2.047297\n1.489693\n...\n0.805058\n1.012419\n0.354864\n0.029351\n0.363899\n1.994053\n0.167231\n8.041667\n8\n4\n\n\n966\n0.908017\n0.161177\n0.633357\n0.037295\n12.691008\n12.879641\n1.859921\n0.413310\n1.884811\n1.501688\n...\n0.841495\n1.014236\n0.189469\n0.063400\n0.086417\n0.641319\n0.203021\n8.050000\n8\n4\n\n\n967\n0.782396\n0.124876\n0.459270\n0.032529\n16.177032\n13.217973\n1.935333\n0.391327\n2.095582\n1.486975\n...\n0.842521\n1.014282\n0.147501\n0.069128\n-0.015460\n0.603300\n0.219220\n8.058333\n8\n4\n\n\n968\n0.694781\n0.147730\n0.524254\n0.045674\n10.516580\n12.007033\n1.704569\n0.380031\n2.034123\n1.495141\n...\n0.832497\n1.013791\n0.160751\n0.077024\n0.331908\n1.264840\n0.247421\n8.066667\n8\n4\n\n\n969\n0.789214\n0.129975\n0.520078\n0.031301\n17.084517\n12.571553\n2.139773\n0.439638\n2.202479\n1.488158\n...\n0.842573\n1.014297\n0.192360\n0.083397\n0.066453\n0.631490\n0.227403\n8.075000\n8\n4\n\n\n\n\n6884 rows × 52 columns\n\n\n\n\nCreación de un reporte .html sobre las variables a utilizar.\n\n\n# Generate automated report and save it to HTML (comentado debido a su gran coste computacional)\n# profile = ProfileReport(sls_pd, title=\"Sleep Stage Data\")\n# profile.to_file(\"eda.html\")\n\n\nDividir el conjunto de datos en Entrenamiento y Validación. Binarizar la variable objetivo.\n\n\n# Create a mapping for sleep stages\nsleep_stages = {\n    0: 'Wake',\n    1: 'N1 sleep',\n    2: 'N2 sleep',\n    3: 'N3 sleep',\n    4: 'REM sleep'\n}\n\n# Train y Validation\nXtrain = sls_pd[sls_pd[\"paciente\"] &lt;= 6].iloc[:,:-3].copy()\n\nytrain = sls_pd[sls_pd[\"paciente\"] &lt;= 6].loc[:,\"y\"].copy()\n\nXval= sls_pd[sls_pd[\"paciente\"] &gt;= 7].iloc[:,:-3].copy()\nyval = sls_pd[sls_pd[\"paciente\"] &gt;= 7].loc[:,\"y\"].copy()\n\n# Binarize the output\nytrain_bin = label_binarize(ytrain, classes=list(sleep_stages.keys()))\nyval_bin = label_binarize(yval, classes=list(sleep_stages.keys()))\n\n\n\n4.3.2 Visualización\nCambiamos los números de la variable objetivo por los nombres de las etapas correspondientes.\n\nsleep_stages = {\n    0: 'Wake',\n    1: 'N1',\n    2: 'N2',\n    3: 'N3',\n    4: 'REM'\n}\n\nsls_pd_tidy = sls_pd.drop([\"time_hour\"], axis=1)\nsls_pd_tidy[\"y\"] = sls_pd_tidy[\"y\"].map(sleep_stages)\nsls_pd_tidy\n\n\n\n\n\n\n\n\neeg_abspow\neeg_alpha\neeg_at\neeg_beta\neeg_db\neeg_ds\neeg_dt\neeg_fdelta\neeg_hcomp\neeg_higuchi\n...\neog_nzc\neog_perm\neog_petrosian\neog_sdelta\neog_sigma\neog_skew\neog_std\neog_theta\npaciente\ny\n\n\nepoch\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n2.223068\n0.306459\n4.183846\n0.126726\n3.420603\n8.966586\n5.917954\n0.240573\n2.320056\n1.823471\n...\n438\n0.932268\n1.020032\n0.379949\n0.017829\n-0.255218\n3.658173\n0.027003\n1\nWake\n\n\n1\n1.739309\n0.214499\n1.858936\n0.142990\n3.273220\n9.813575\n4.056213\n0.301374\n2.060777\n1.813796\n...\n489\n0.930587\n1.019913\n0.294492\n0.036311\n-0.012890\n2.873480\n0.048089\n1\nWake\n\n\n2\n10.646273\n0.053756\n1.102060\n0.103112\n7.565166\n23.103180\n15.992047\n0.138216\n2.405262\n1.746981\n...\n559\n0.943526\n1.020977\n0.443489\n0.052026\n0.080295\n3.556994\n0.042671\n1\nWake\n\n\n3\n3.218050\n0.125759\n1.339224\n0.195343\n2.857057\n8.191535\n5.943351\n0.364850\n2.384103\n1.798444\n...\n719\n0.949502\n1.021550\n0.403663\n0.043639\n-0.410182\n2.967387\n0.048401\n1\nWake\n\n\n4\n1.839396\n0.145482\n1.453737\n0.268479\n1.486379\n4.766511\n3.987657\n0.157197\n2.086368\n1.840072\n...\n795\n0.951833\n1.021755\n0.187960\n0.063337\n0.131732\n2.672432\n0.039588\n1\nWake\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n965\n1.065989\n0.146153\n0.608933\n0.037588\n14.911792\n14.893099\n2.335285\n0.440802\n2.047297\n1.489693\n...\n293\n0.805058\n1.012419\n0.354864\n0.029351\n0.363899\n1.994053\n0.167231\n8\nREM\n\n\n966\n0.908017\n0.161177\n0.633357\n0.037295\n12.691008\n12.879641\n1.859921\n0.413310\n1.884811\n1.501688\n...\n426\n0.841495\n1.014236\n0.189469\n0.063400\n0.086417\n0.641319\n0.203021\n8\nREM\n\n\n967\n0.782396\n0.124876\n0.459270\n0.032529\n16.177032\n13.217973\n1.935333\n0.391327\n2.095582\n1.486975\n...\n424\n0.842521\n1.014282\n0.147501\n0.069128\n-0.015460\n0.603300\n0.219220\n8\nREM\n\n\n968\n0.694781\n0.147730\n0.524254\n0.045674\n10.516580\n12.007033\n1.704569\n0.380031\n2.034123\n1.495141\n...\n380\n0.832497\n1.013791\n0.160751\n0.077024\n0.331908\n1.264840\n0.247421\n8\nREM\n\n\n969\n0.789214\n0.129975\n0.520078\n0.031301\n17.084517\n12.571553\n2.139773\n0.439638\n2.202479\n1.488158\n...\n403\n0.842573\n1.014297\n0.192360\n0.083397\n0.066453\n0.631490\n0.227403\n8\nREM\n\n\n\n\n6884 rows × 51 columns\n\n\n\nHacemos el data frame tidy para poder realizar gráficos exploratorios.\n\nsls_pd_tidy = sls_pd_tidy.melt(id_vars = [\"paciente\",\"y\"], value_vars = sls_pd_tidy.columns[:-2])\nsls_pd_tidy\n\n\n\n\n\n\n\n\npaciente\ny\nvariable\nvalue\n\n\n\n\n0\n1\nWake\neeg_abspow\n2.223068\n\n\n1\n1\nWake\neeg_abspow\n1.739309\n\n\n2\n1\nWake\neeg_abspow\n10.646273\n\n\n3\n1\nWake\neeg_abspow\n3.218050\n\n\n4\n1\nWake\neeg_abspow\n1.839396\n\n\n...\n...\n...\n...\n...\n\n\n337311\n8\nREM\neog_theta\n0.167231\n\n\n337312\n8\nREM\neog_theta\n0.203021\n\n\n337313\n8\nREM\neog_theta\n0.219220\n\n\n337314\n8\nREM\neog_theta\n0.247421\n\n\n337315\n8\nREM\neog_theta\n0.227403\n\n\n\n\n337316 rows × 4 columns\n\n\n\nRealizamos un boxplot para las variables eeg_abspow y eog_skew para ver las diferencias que muestran en cada fase del sueño. Este gráfico, entre otros, se mostrará en el dashboard final al que tendrá acceso el médico.\n\ng = sns.catplot(data=sls_pd_tidy[sls_pd_tidy[\"variable\"].isin([\"eeg_abspow\",\"eog_skew\"])], y=\"value\", hue=\"y\", col=\"variable\", kind = \"box\", sharey=False)\n# Eliminar la leyenda automática\ng._legend.remove()\n# Agregar la leyenda personalizada\nplt.legend(ncol=5, loc=\"lower center\", bbox_to_anchor=(0, -0.1))\nplt.show()\n\n\n\n\n\n\n\n\nPara la variable eeg_abspow observamos cómo obtiene valores más altos y mucha mayor varianza para las fases Wake y N3, mientras que para las otras muestra valores relativamente similares y una menor varianza.\nPara la variable eog_skew los valores son muy similares para todas las fases del sueño. La diferencia radica en la varianza en cada fase. Se puede observar cómo para la fase N3 la varianza es notablemente menor que en las demás, mientras que en las fase REM y, en menor medida, en la fase Wake observamos una varianza muy elevada.\nAhora realizamos una matriz de correlación que muestra las correlaciones que tienen las variables del dataset entre sí.\n\nplt.imshow(np.corrcoef(Xtrain, rowvar = False), cmap =\"viridis\")\nplt.show()\n\n\n\n\n\n\n\n\nAparte de la obvia correlación máxima que tiene cada variable consigo misma (diagonal principal), observamos varios grupos de variables con correlaciones muy altas, como se puede observar en los cuadrados amarillos (correlación positiva) y los morados (correlación negativa)."
  },
  {
    "objectID": "posts/Fases del Suenyo/index.html#algoritmo-xgboost-para-clasificación-de-etapas-del-sueño",
    "href": "posts/Fases del Suenyo/index.html#algoritmo-xgboost-para-clasificación-de-etapas-del-sueño",
    "title": "Detección de las Fases del Sueño mediante XGBoost.",
    "section": "5.1 Algoritmo XGBoost para Clasificación de Etapas del Sueño",
    "text": "5.1 Algoritmo XGBoost para Clasificación de Etapas del Sueño\nXGBoost es una implementación específica de Random Forest con gradient boosting, que incluye diversas optimizaciones para mejorar el rendimiento y la precisión. Sus ventajas principales incluyen:\n\nRegularización: Incorpora términos de regularización L1 (Lasso) y L2 (Ridge) para prevenir el sobreajuste, algo que no está presente en la implementación de GradientBoosting de scikit-learn.\n\nManejo de valores faltantes: XGBoost tiene capacidad integrada para manejar datos faltantes, a diferencia de la implementación de scikit-learn.\n\nParalelización: Está diseñado para trabajar de manera eficiente en sistemas paralelos y distribuidos, lo que lo hace mucho más rápido que el Boosting estándar de scikit-learn.\n\nPoda de árboles: Utiliza una técnica llamada “poda de profundidad máxima” (max depth pruning), que detiene el crecimiento de los árboles cuando ya no aportan valor.\n\nEste enfoque combina velocidad, robustez y flexibilidad, haciéndolo ideal para tareas complejas como la clasificación de etapas del sueño.\n\n5.1.1 Interpretabilidad\nPara mejorar la interpretabilidad del modelo, se empleará un XGBoost para cada estado de sueño. De este modo, se obtendrá de forma más precisa qué características ha utilizado cada modelo para realizar la predicción. Además, para cada modelo se ha realizado también una version surrogate mediante un árbol de decisión. Este modelo es menos potente que el XGBoost, pero ayuda a la interpretabilidad.\nCabe destacar que se ha realizado un grid search exhaustivo para cada modelo, pero se ha comentado debido a su gran coste computacional. Los parámetros elegidos para cada modelo son aquellos obtenidos mediante el grid search. Se han probado un total de 6 parámetros diferentes, con 3 posibles valores para cada uno:\n\nlearning_rate: Se trata de la velocidad a la que aprende el modelo. Un valor más alto indica una velocidad de aprendizaje mayor. No es recomendable que su valor sea excesivamente alto por el riesgo de overfitting\n\n\nmax_depth: Profundidad máxima que puede tener cada árbol. Una profundidad demasiado elevada puede crear overfitting, pero una demasiado baja puede no captar todas las características de los datos.\n\n\nmin_child_weight: Es la suma mínima de los pesos de las instancias requeridos para dividir un nodo. Nodos con menor valor del peso establecido no serán divididos.\n\n\nn_estimators: Representa el número de árboles que serán entrenados en el modelo. Un número mayor de árboles puede captar más detalles, pero existe el riesgo de overfitting.\n\n\nreg_alpha: Controla la regularización L1, que penaliza las características menos relevantes al reducir los pesos asociados.\n\n\nreg_lambda: Controla la regularización L2, que penaliza los pesos grandes para evitar que el modelo dependa demasiado de características individuales.\n\nPrimero creamos dos funciones que nos ayudarán a interpretar los resultados:\n\nshow_features: Muestra las 20 variables más importantes para el modelo en orden descendente en base a su cover (número de veces que se utiliza la variable para hacer divisiones en los árboles) y su gain (mejora o reducción del error media de la variable cuando se utiliza para dividir).\n\n\nshow_metricas: Muestra un total de 8 métricas para analizar los resultados del modelo.\n\n\ndef show_features(model, max_num_features=20):\n    fig, axes = plt.subplots(1, 2, sharex=False, sharey=False, figsize = (20,5))\n    contx = 0\n\n    for type in [\"cover\",\"gain\"]:\n        xgb.plot_importance(model, importance_type=type, max_num_features=max_num_features, ax=axes[contx],values_format=\"{v:.2f}\")\n        axes[contx].set_title(type)\n        axes[contx].set_ylabel(\"\")\n        contx += 1\n    fig.suptitle(\"Feature Importance\")\n    fig.supylabel(\"Features\")\n    plt.show()\n    return\n\ndef show_metricas(n_modelo,y_true, y_pred,multi=False):\n    \n    if multi:\n        modo = \"weighted\"\n        auc = None\n        esp = None\n    else:\n        modo = \"binary\"\n        auc = metrics.roc_auc_score(y_true, y_pred)\n        esp = metrics.recall_score(y_true, y_pred, pos_label=0)\n\n    metricas = {\"Modelo\":n_modelo,\n                \"Accuracy\":metrics.accuracy_score(y_true, y_pred),\n                \"Precision\":metrics.precision_score(y_true, y_pred, average=modo),\n                \"AP\":metrics.average_precision_score(y_true, y_pred),\n                \"AUC\":auc,\n                \"Sensibilidad\":metrics.recall_score(y_true, y_pred, average=modo),\n                \"Especificidad\":esp,\n                \"F-Score\":metrics.f1_score(y_true,y_pred,average=modo)}\n    return metricas\n\nmodelos = []\nmetricas = []"
  },
  {
    "objectID": "posts/Fases del Suenyo/index.html#modelo-wake",
    "href": "posts/Fases del Suenyo/index.html#modelo-wake",
    "title": "Detección de las Fases del Sueño mediante XGBoost.",
    "section": "5.2 Modelo Wake",
    "text": "5.2 Modelo Wake\n\n5.2.1 Creación del modelo\nPrimero realizamos el modelo Wake, que distinguirá cuándo el paciente está despierto (Wake) o dormido.\nLos parámetros elegidos por el grid search son:\n\nlearning_rate = 0.4\n\n\nmax_depth = 5\n\n\nmin_child_weight = 1.0\n\n\nn_estimators = 1000\n\n\nreg_alpha = 0.5\n\n\nreg_lambda = 3\n\n\nmodelWake = xgb.XGBClassifier(learning_rate=0.4, \n                              max_depth=5, \n                              min_child_weight=1.0, \n                              n_estimators=1000, \n                              reg_alpha=0.5, \n                              reg_lambda=3)\n\"\"\"\nparam_grid = {\n    'n_estimators': [1000, 3000, 5000],\n    'max_depth': [4, 5, 6],\n    'learning_rate': [0.1, 0.3, 0.4],\n    'min_child_weight': [1.0, 1.5, 5],\n    'reg_alpha': [0, 0.1, 0.5],\n    'reg_lambda': [1, 2, 3]\n}\n\ngrid_search = GridSearchCV(\n    estimator=modelWake,\n    param_grid=param_grid,\n    scoring='accuracy',\n    cv=3,\n    verbose=1,\n    n_jobs=-1\n)\n\ngrid_search.fit(Xtrain,ytrain_bin[:,0])\n\nprint(\"Best Parameters:\", grid_search.best_params_)\nprint(\"Best Cross-Validation Accuracy:\", grid_search.best_score_)\n\nmodelWake = grid_search.best_estimator_\n\"\"\"\nmodelWake.fit(Xtrain,ytrain_bin[:,0])\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              feature_weights=None, gamma=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=0.4, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=5,\n              max_leaves=None, min_child_weight=1.0, missing=nan,\n              monotone_constraints=None, multi_strategy=None, n_estimators=1000,\n              n_jobs=None, num_parallel_tree=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBClassifier?Documentation for XGBClassifieriFitted\n        \n            \n                Parameters\n                \n\n\n\n\nobjective \n'binary:logistic'\n\n\n\nbase_score \nNone\n\n\n\nbooster \nNone\n\n\n\ncallbacks \nNone\n\n\n\ncolsample_bylevel \nNone\n\n\n\ncolsample_bynode \nNone\n\n\n\ncolsample_bytree \nNone\n\n\n\ndevice \nNone\n\n\n\nearly_stopping_rounds \nNone\n\n\n\nenable_categorical \nFalse\n\n\n\neval_metric \nNone\n\n\n\nfeature_types \nNone\n\n\n\nfeature_weights \nNone\n\n\n\ngamma \nNone\n\n\n\ngrow_policy \nNone\n\n\n\nimportance_type \nNone\n\n\n\ninteraction_constraints \nNone\n\n\n\nlearning_rate \n0.4\n\n\n\nmax_bin \nNone\n\n\n\nmax_cat_threshold \nNone\n\n\n\nmax_cat_to_onehot \nNone\n\n\n\nmax_delta_step \nNone\n\n\n\nmax_depth \n5\n\n\n\nmax_leaves \nNone\n\n\n\nmin_child_weight \n1.0\n\n\n\nmissing \nnan\n\n\n\nmonotone_constraints \nNone\n\n\n\nmulti_strategy \nNone\n\n\n\nn_estimators \n1000\n\n\n\nn_jobs \nNone\n\n\n\nnum_parallel_tree \nNone\n\n\n\nrandom_state \nNone\n\n\n\nreg_alpha \n0.5\n\n\n\nreg_lambda \n3\n\n\n\nsampling_method \nNone\n\n\n\nscale_pos_weight \nNone\n\n\n\nsubsample \nNone\n\n\n\ntree_method \nNone\n\n\n\nvalidate_parameters \nNone\n\n\n\nverbosity \nNone\n\n\n\n\n            \n        \n    \n\n\n\n\n5.2.2 Selección de características\nPrimero, utilizaremos los valores de Shapley (Shap) para obtener las variables más importantes y cómo afectan estas al modelo.\n\nexplainer = shap.TreeExplainer(modelWake)\nshap_values_Wake = explainer.shap_values(Xtrain)\nshap_values_Wake.shape\n\n(5130, 49)\n\n\n\nshap.summary_plot(shap_values_Wake, Xtrain, show=False)\n\n\n\n\n\n\n\n\nObtenemos que eeg_fdelta, eeg_higuchi y eeg_at son las variables más importantes para el modelo Wake.\nEstas variables afectan al modelo de la siguiente manera, interpretando el resultado obtenido:\n\neeg_fdelta: Los valores más altos de esta variable afectan negativamente, es decir, si se tiene un valor de eeg_fdelta alto, la predicción del modelo será que el paciente está dormido.\n\n\neeg_higuchi: Al contrario que en el caso anterior, los valores más altos significarán que el paciente está despierto, y los más bajos, que está durmiendo.\n\n\neeg_at: Al igual que en la variable eeg_higuchi, pero de manera menos clara, los valores más altos de esta variable indicarán que el paciente está despierto.\n\nAhora mostramos por pantalla las variables más importantes para el modelo y observamos si coinciden.\n\nshow_features(modelWake)\n\n\n\n\n\n\n\n\nObservamos que entre las variables más importantes y, por tanto, las que mejor explican este modelo están eeg_higuchi y eeg_fdelta, al igual que con Shap. Sin embargo, vemos cómo eeg_at no es considerada tan importante y, en su lugar, emg_std sí que lo es.\n\n\n5.2.3 Uso de Surrogate model para la interpretabilidad\n\nsurrogateWake_labels = modelWake.predict(Xtrain)\nsurrogateWake_model = DecisionTreeClassifier(random_state=42, max_depth=5)\nsurrogateWake_model.fit(Xtrain,surrogateWake_labels)\n\nplt.figure(figsize=(20,10))\nplot_tree(surrogateWake_model, filled=True, feature_names=Xtrain.columns, class_names=[\"Sleep\",\"Wake\"])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n5.2.4 Métricas\n\nmetricas_Wake = show_metricas(\"Wake\",yval_bin[:,0],modelWake.predict(Xval))\nmetricas.append(metricas_Wake)\nmetricas_Wake\n\n{'Modelo': 'Wake',\n 'Accuracy': 0.9629418472063854,\n 'Precision': 0.9708029197080292,\n 'AP': 0.9168641206245217,\n 'AUC': 0.9510112059491161,\n 'Sensibilidad': 0.9156626506024096,\n 'Especificidad': 0.9863597612958227,\n 'F-Score': 0.9424269264836138}\n\n\nEl modelo obtenido ofrece unas métricas muy buenas, todas ellas por encima de 0.9, con una gran sensibilidad y especificidad.\n\nmodelos.append(modelWake)"
  },
  {
    "objectID": "posts/Fases del Suenyo/index.html#modelo-n1",
    "href": "posts/Fases del Suenyo/index.html#modelo-n1",
    "title": "Detección de las Fases del Sueño mediante XGBoost.",
    "section": "5.3 Modelo N1",
    "text": "5.3 Modelo N1\n\n5.3.1 Creación del modelo\nAhora realizamos un modelo para la fase N1 o etapa I del sueño. Esta etapa tiene menos características que la hagan única, así que será más complicado predecirla correctamente.\nLos parámetros obtenidos en el grid search son:\n\nlearning_rate = 0.4\n\n\nmax_depth = 6\n\n\nmin_child_weight = 1.0\n\n\nn_estimators = 1000\n\n\nreg_alpha = 0.5\n\n\nreg_lambda = 1\n\n\nmodelN1 = xgb.XGBClassifier(learning_rate=0.4, \n                            max_depth=6, \n                            min_child_weight=1.0, \n                            n_estimators=1000, \n                            reg_alpha=0.5, \n                            reg_lambda=1)\n\"\"\"\nparam_grid = {\n    'n_estimators': [1000, 3000, 5000],\n    'max_depth': [4, 5, 6],\n    'learning_rate': [0.1, 0.3, 0.4],\n    'min_child_weight': [1.0, 1.5, 5],\n    'reg_alpha': [0, 0.1, 0.5],\n    'reg_lambda': [1, 2, 3]\n}\n\ngrid_search = GridSearchCV(\n    estimator=modelN1,\n    param_grid=param_grid,\n    scoring='accuracy',\n    cv=3,\n    verbose=1,\n    n_jobs=-1\n)\n\ngrid_search.fit(Xtrain,ytrain_bin[:,1])\n\nprint(\"Best Parameters:\", grid_search.best_params_)\nprint(\"Best Cross-Validation Accuracy:\", grid_search.best_score_)\n\nmodelN1 = grid_search.best_estimator_\n\"\"\"\nmodelN1.fit(Xtrain,ytrain_bin[:,1])\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              feature_weights=None, gamma=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=0.4, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n              max_leaves=None, min_child_weight=1.0, missing=nan,\n              monotone_constraints=None, multi_strategy=None, n_estimators=1000,\n              n_jobs=None, num_parallel_tree=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBClassifier?Documentation for XGBClassifieriFitted\n        \n            \n                Parameters\n                \n\n\n\n\nobjective \n'binary:logistic'\n\n\n\nbase_score \nNone\n\n\n\nbooster \nNone\n\n\n\ncallbacks \nNone\n\n\n\ncolsample_bylevel \nNone\n\n\n\ncolsample_bynode \nNone\n\n\n\ncolsample_bytree \nNone\n\n\n\ndevice \nNone\n\n\n\nearly_stopping_rounds \nNone\n\n\n\nenable_categorical \nFalse\n\n\n\neval_metric \nNone\n\n\n\nfeature_types \nNone\n\n\n\nfeature_weights \nNone\n\n\n\ngamma \nNone\n\n\n\ngrow_policy \nNone\n\n\n\nimportance_type \nNone\n\n\n\ninteraction_constraints \nNone\n\n\n\nlearning_rate \n0.4\n\n\n\nmax_bin \nNone\n\n\n\nmax_cat_threshold \nNone\n\n\n\nmax_cat_to_onehot \nNone\n\n\n\nmax_delta_step \nNone\n\n\n\nmax_depth \n6\n\n\n\nmax_leaves \nNone\n\n\n\nmin_child_weight \n1.0\n\n\n\nmissing \nnan\n\n\n\nmonotone_constraints \nNone\n\n\n\nmulti_strategy \nNone\n\n\n\nn_estimators \n1000\n\n\n\nn_jobs \nNone\n\n\n\nnum_parallel_tree \nNone\n\n\n\nrandom_state \nNone\n\n\n\nreg_alpha \n0.5\n\n\n\nreg_lambda \n1\n\n\n\nsampling_method \nNone\n\n\n\nscale_pos_weight \nNone\n\n\n\nsubsample \nNone\n\n\n\ntree_method \nNone\n\n\n\nvalidate_parameters \nNone\n\n\n\nverbosity \nNone\n\n\n\n\n            \n        \n    \n\n\n\n\n5.3.2 Selección de características\nPrimero, utilizaremos los valores de Shapley (Shap) para obtener las variables más importantes y cómo afectan estas al modelo.\n\nexplainer = shap.TreeExplainer(modelN1)\nshap_values_N1 = explainer.shap_values(Xtrain)\nshap_values_N1.shape\n\n(5130, 49)\n\n\n\nshap.summary_plot(shap_values_N1, Xtrain, show=False)\n\n\n\n\n\n\n\n\nObtenemos que eeg_beta, eeg_abspow y eeg_iqr son las variables más importantes para el modelo N1.\nEstas variables afectan al modelo de la siguiente manera, interpretando el resultado obtenido:\n\neeg_beta: Los valores más bajos de esta variable afectan negativamente, es decir, si se tiene un valor de eeg_beta bajo, la predicción del modelo será que el paciente no está en la fase N1, y si el valor es alto, entonces sí que lo estará.\n\n\neeg_abspow: Al contrario que en el caso anterior, aunque con muchas más excepciones, los valores más bajos significarán que el paciente está en la fase N1, y los más bajos, que no lo está.\n\n\neeg_iqr: Al igual que en la variable eeg_abspow, y con menos excpeciones en los valores extremos, los valores más bajos de esta variable indicarán que el paciente está en la fase N1.\n\nAhora mostramos las variables más importantes para el modelo y observamos si coinciden con el resultado obtenido en Shap.\n\nshow_features(modelN1)\n\n\n\n\n\n\n\n\nObtenemos muchas variables con importancia similar en el modelo. Las más importantes es eeg_abspow, con mucho gain, y eeg_beta, con mucho cover, que ya habían aparecido en Shap. Sin embargo, la variable eeg_iqr no es considerada tan importante en este caso, y tenemos otras variables como emg_hmob, con mucho cover, y eeg_theta, con mucho gain en su lugar.\n\n\n5.3.3 Uso de Surrogate model para la interpretabilidad\n\nsurrogateN1_labels = modelN1.predict(Xtrain)\nsurrogateN1_model = DecisionTreeClassifier(random_state=42, max_depth=5)\nsurrogateN1_model.fit(Xtrain,surrogateN1_labels)\n\nplt.figure(figsize=(20,10))\nplot_tree(surrogateN1_model, filled=True, feature_names=Xtrain.columns, class_names=[\"No N1\",\"N1\"])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n5.3.4 Métricas\n\nmetricas_N1 = show_metricas(\"N1\",yval_bin[:,1],modelN1.predict_proba(Xval)[:,1]&gt;0.3)\nmetricas.append(metricas_N1)\nmetricas_N1\n\n{'Modelo': 'N1',\n 'Accuracy': 0.9076396807297605,\n 'Precision': 0.55,\n 'AP': 0.24257676805176212,\n 'AUC': 0.6456612177696764,\n 'Sensibilidad': 0.31976744186046513,\n 'Especificidad': 0.9715549936788875,\n 'F-Score': 0.40441176470588236}\n\n\nTal y como suponíamos, es bastante complicado obtener grandes resultados para esta etapa. Pese a tener un muy buen accuracy y especificidad, sus valores para el AP y la sensibilidad son muy bajos. Por lo tanto, se puede concluir que el rendimiento de este modelo es bastante inferior al del modelo anterior.\n\nmodelos.append(modelN1)"
  },
  {
    "objectID": "posts/Fases del Suenyo/index.html#modelo-n2",
    "href": "posts/Fases del Suenyo/index.html#modelo-n2",
    "title": "Detección de las Fases del Sueño mediante XGBoost.",
    "section": "5.4 Modelo N2",
    "text": "5.4 Modelo N2\n\n5.4.1 Creación del modelo\nAhora realizamos un modelo para la fase N2 o etapa II del sueño. Esta etapa es más reconocible que la I, pero sigue teniendo pocas características únicas.\nLos parámetros obtenidos en el grid search son:\n\nlearning_rate = 0.3\n\n\nmax_depth = 5\n\n\nmin_child_weight = 1.5\n\n\nn_estimators = 5000\n\n\nreg_alpha = 0\n\n\nreg_lambda = 3\n\n\nmodelN2 = xgb.XGBClassifier(learning_rate=0.3, \n                            max_depth=5, \n                            min_child_weight=1.5, \n                            n_estimators=5000, \n                            reg_alpha=0, \n                            reg_lambda=3)\n\"\"\"\nparam_grid = {\n    'n_estimators': [1000, 3000, 5000],\n    'max_depth': [4, 5, 6],\n    'learning_rate': [0.1, 0.3, 0.4],\n    'min_child_weight': [1.0, 1.5, 5],\n    'reg_alpha': [0, 0.1, 0.5],\n    'reg_lambda': [1, 2, 3]\n}\n\ngrid_search = GridSearchCV(\n    estimator=modelN2,\n    param_grid=param_grid,\n    scoring='accuracy',\n    cv=3,\n    verbose=1,\n    n_jobs=-1\n)\n\ngrid_search.fit(Xtrain,ytrain_bin[:,2])\n\nprint(\"Best Parameters:\", grid_search.best_params_)\nprint(\"Best Cross-Validation Accuracy:\", grid_search.best_score_)\n\nmodelN2 = grid_search.best_estimator_\n\"\"\"\nmodelN2.fit(Xtrain,ytrain_bin[:,2])\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              feature_weights=None, gamma=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=5,\n              max_leaves=None, min_child_weight=1.5, missing=nan,\n              monotone_constraints=None, multi_strategy=None, n_estimators=5000,\n              n_jobs=None, num_parallel_tree=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBClassifier?Documentation for XGBClassifieriFitted\n        \n            \n                Parameters\n                \n\n\n\n\nobjective \n'binary:logistic'\n\n\n\nbase_score \nNone\n\n\n\nbooster \nNone\n\n\n\ncallbacks \nNone\n\n\n\ncolsample_bylevel \nNone\n\n\n\ncolsample_bynode \nNone\n\n\n\ncolsample_bytree \nNone\n\n\n\ndevice \nNone\n\n\n\nearly_stopping_rounds \nNone\n\n\n\nenable_categorical \nFalse\n\n\n\neval_metric \nNone\n\n\n\nfeature_types \nNone\n\n\n\nfeature_weights \nNone\n\n\n\ngamma \nNone\n\n\n\ngrow_policy \nNone\n\n\n\nimportance_type \nNone\n\n\n\ninteraction_constraints \nNone\n\n\n\nlearning_rate \n0.3\n\n\n\nmax_bin \nNone\n\n\n\nmax_cat_threshold \nNone\n\n\n\nmax_cat_to_onehot \nNone\n\n\n\nmax_delta_step \nNone\n\n\n\nmax_depth \n5\n\n\n\nmax_leaves \nNone\n\n\n\nmin_child_weight \n1.5\n\n\n\nmissing \nnan\n\n\n\nmonotone_constraints \nNone\n\n\n\nmulti_strategy \nNone\n\n\n\nn_estimators \n5000\n\n\n\nn_jobs \nNone\n\n\n\nnum_parallel_tree \nNone\n\n\n\nrandom_state \nNone\n\n\n\nreg_alpha \n0\n\n\n\nreg_lambda \n3\n\n\n\nsampling_method \nNone\n\n\n\nscale_pos_weight \nNone\n\n\n\nsubsample \nNone\n\n\n\ntree_method \nNone\n\n\n\nvalidate_parameters \nNone\n\n\n\nverbosity \nNone\n\n\n\n\n            \n        \n    \n\n\n\n\n5.4.2 Selección de características\nPrimero, utilizaremos los valores de Shapley (Shap) para obtener las variables más importantes y cómo afectan estas al modelo.\n\nexplainer = shap.TreeExplainer(modelN2)\nshap_values_N2 = explainer.shap_values(Xtrain)\nshap_values_N2.shape\n\n(5130, 49)\n\n\n\nshap.summary_plot(shap_values_N2, Xtrain, show=False)\n\n\n\n\n\n\n\n\nObtenemos que eeg_abspow, eeg_kurt y eeg_fdelta son las variables más importantes para el modelo N2.\nEstas variables afectan al modelo de la siguiente manera, interpretando el resultado obtenido:\n\neeg_abspow: Los valores más altos de esta variable afectan positivamente, es decir, si se tiene un valor de eeg_abspow alto, la predicción del modelo será que el paciente está en la fase N2.\n\n\neeg_kurt: De la misma manera que en el caso anterior, los valores más altos significarán que el paciente está en la fase N2, y los más bajos, que no lo está.\n\n\neeg_fdelta: Al igual que en las variables anteriores, pero con mayor claridad en los valores extremos, los valores más altos de esta variable indicarán que el paciente está en la fase N2.\n\nAhora mostramos las variables más importantes para el modelo y observamos si coinciden con el resultado obtenido en Shap.\n\nshow_features(modelN2)\n\n\n\n\n\n\n\n\nPor primera vez obtenemos variables completamente distintas en Shap y en cover y gain. Pese a que este modelo tiene bastantes de importancia similar, ninguna de las principales es eeg_abspow, eeg_kurt o eeg_fdelta. Las que más gain producen son eeg_db, eog_theta, eog_iqr y eeg_std, y las que más cover producen son eeg_db, eog_alpha y eog_std.\n\n\n5.4.3 Uso de Surrogate model para la interpretabilidad\n\nsurrogateN2_labels = modelN2.predict(Xtrain)\nsurrogateN2_model = DecisionTreeClassifier(random_state=42, max_depth=6)\nsurrogateN2_model.fit(Xtrain,surrogateN2_labels)\n\nplt.figure(figsize=(20,10))\nplot_tree(surrogateN2_model, filled=True, feature_names=Xtrain.columns, class_names=[\"No N2\",\"N2\"])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n5.4.4 Métricas\n\nmetricas_N2 = show_metricas(\"N2\",yval_bin[:,2],modelN2.predict(Xval))\nmetricas.append(metricas_N2)\nmetricas_N2\n\n{'Modelo': 'N2',\n 'Accuracy': 0.8603192702394526,\n 'Precision': 0.6145374449339207,\n 'AP': 0.531186565423172,\n 'AUC': 0.8374358869775361,\n 'Sensibilidad': 0.7994269340974212,\n 'Especificidad': 0.8754448398576512,\n 'F-Score': 0.6948941469489415}\n\n\nEl resultado obtenido es bastante decente. Pese a tener una precisión bastante baja, su accuracy, su sensibilidad y su especificidad son bastante buenos (&gt;0.8), por lo tanto se puede concluir que se trata de un modelo bastante bueno.\n\nmodelos.append(modelN2)"
  },
  {
    "objectID": "posts/Fases del Suenyo/index.html#modelo-n3",
    "href": "posts/Fases del Suenyo/index.html#modelo-n3",
    "title": "Detección de las Fases del Sueño mediante XGBoost.",
    "section": "5.5 Modelo N3",
    "text": "5.5 Modelo N3\n\n5.5.1 Creación del modelo\nEl siguiente modelo tratará de predecir la fase N3 o slow wave, bastante reconocible debido a la gran amplitud que muestran los distintos valores medidos.\nLos parámetros obtenidos en el grid search son:\n\nlearning_rate = 0.4\n\n\nmax_depth = 6\n\n\nmin_child_weight = 1.0\n\n\nn_estimators = 1000\n\n\nreg_alpha = 0.5\n\n\nreg_lambda = 1\n\n\nmodelN3 = xgb.XGBClassifier(learning_rate=0.4, \n                            max_depth=6, \n                            min_child_weight=1.0, \n                            n_estimators=1000, \n                            reg_alpha=0.5, \n                            reg_lambda=1)\n\"\"\"\nparam_grid = {\n    'n_estimators': [1000, 3000, 5000],\n    'max_depth': [4, 5, 6],\n    'learning_rate': [0.1, 0.3, 0.4],\n    'min_child_weight': [1.0, 1.5, 5],\n    'reg_alpha': [0, 0.1, 0.5],\n    'reg_lambda': [1, 2, 3]\n}\n\ngrid_search = GridSearchCV(\n    estimator=modelN3,\n    param_grid=param_grid,\n    scoring='accuracy',\n    cv=3,\n    verbose=1,\n    n_jobs=-1\n)\n\ngrid_search.fit(Xtrain,ytrain_bin[:,3])\n\nprint(\"Best Parameters:\", grid_search.best_params_)\nprint(\"Best Cross-Validation Accuracy:\", grid_search.best_score_)\nmodelN3 = grid_search.best_estimator_\n\"\"\"\nmodelN3.fit(Xtrain,ytrain_bin[:,3])\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              feature_weights=None, gamma=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=0.4, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n              max_leaves=None, min_child_weight=1.0, missing=nan,\n              monotone_constraints=None, multi_strategy=None, n_estimators=1000,\n              n_jobs=None, num_parallel_tree=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBClassifier?Documentation for XGBClassifieriFitted\n        \n            \n                Parameters\n                \n\n\n\n\nobjective \n'binary:logistic'\n\n\n\nbase_score \nNone\n\n\n\nbooster \nNone\n\n\n\ncallbacks \nNone\n\n\n\ncolsample_bylevel \nNone\n\n\n\ncolsample_bynode \nNone\n\n\n\ncolsample_bytree \nNone\n\n\n\ndevice \nNone\n\n\n\nearly_stopping_rounds \nNone\n\n\n\nenable_categorical \nFalse\n\n\n\neval_metric \nNone\n\n\n\nfeature_types \nNone\n\n\n\nfeature_weights \nNone\n\n\n\ngamma \nNone\n\n\n\ngrow_policy \nNone\n\n\n\nimportance_type \nNone\n\n\n\ninteraction_constraints \nNone\n\n\n\nlearning_rate \n0.4\n\n\n\nmax_bin \nNone\n\n\n\nmax_cat_threshold \nNone\n\n\n\nmax_cat_to_onehot \nNone\n\n\n\nmax_delta_step \nNone\n\n\n\nmax_depth \n6\n\n\n\nmax_leaves \nNone\n\n\n\nmin_child_weight \n1.0\n\n\n\nmissing \nnan\n\n\n\nmonotone_constraints \nNone\n\n\n\nmulti_strategy \nNone\n\n\n\nn_estimators \n1000\n\n\n\nn_jobs \nNone\n\n\n\nnum_parallel_tree \nNone\n\n\n\nrandom_state \nNone\n\n\n\nreg_alpha \n0.5\n\n\n\nreg_lambda \n1\n\n\n\nsampling_method \nNone\n\n\n\nscale_pos_weight \nNone\n\n\n\nsubsample \nNone\n\n\n\ntree_method \nNone\n\n\n\nvalidate_parameters \nNone\n\n\n\nverbosity \nNone\n\n\n\n\n            \n        \n    \n\n\n\n\n5.5.2 Selección de características\nPrimero, utilizaremos los valores de Shapley (Shap) para obtener las variables más importantes y cómo afectan estas al modelo.\n\nexplainer = shap.TreeExplainer(modelN3)\nshap_values_N3 = explainer.shap_values(Xtrain)\nshap_values_N3.shape\n\n(5130, 49)\n\n\n\nshap.summary_plot(shap_values_N3, Xtrain, show=False)\n\n\n\n\n\n\n\n\nObtenemos que eeg_db, eeg_iqr y eog_higuchi son las variables más importantes para el modelo N3.\nEstas variables afectan al modelo de la siguiente manera, interpretando el resultado obtenido:\n\neeg_db: Los valores más bajos de esta variable afectan negativamente de manera bastante clara, es decir, si se tiene un valor de eeg_db bajo, la predicción del modelo será que el paciente no está en la fase N3, mientras que si el valor es muy alto, significará que sí que lo está.\n\n\neeg_iqr: Al igual que en el caso anterior, los valores más altos significarán que el paciente está en la fase N3, y los más bajos, que no lo está.\n\n\neog_higuchi: Al contrario que en las variables anteriores, los valores más altos de esta variable indicarán que el paciente no está en la fase N3, y lo más altos, que sí.\n\nAhora mostramos las variables más importantes para el modelo y observamos si coinciden con el resultado obtenido en Shap.\n\nshow_features(modelN3)\n\n\n\n\n\n\n\n\nCon mucha diferencia, la variable más importante para este modelo es eeg_db, que coincide con el resultado obtenido en Shap, aunque también es bastante importante eog_petrosian, la cual aparece bastante más abajo en Shap, pero con una larga cola que indica que los valores más bajos están en la fase N3. La siguiente más importante es, al igual que en Shap, eeg_iqr.\n\n\n5.5.3 Uso de Surrogate model para la interpretabilidad\n\nsurrogateN3_labels = modelN3.predict(Xtrain)\nsurrogateN3_model = DecisionTreeClassifier(random_state=42, max_depth=6)\nsurrogateN3_model.fit(Xtrain,surrogateN3_labels)\n\nplt.figure(figsize=(20,10))\nplot_tree(surrogateN3_model, filled=True, feature_names=Xtrain.columns, class_names=[\"No N3\",\"N3\"])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n5.5.4 Métricas\n\nmetricas_N3 = show_metricas(\"N3\",yval_bin[:,3],modelN3.predict(Xval))\nmetricas.append(metricas_N3)\nmetricas_N3\n\n{'Modelo': 'N3',\n 'Accuracy': 0.9464082098061574,\n 'Precision': 0.8729016786570744,\n 'AP': 0.8079089969293201,\n 'AUC': 0.9297385351746607,\n 'Sensibilidad': 0.8987654320987655,\n 'Especificidad': 0.9607116382505559,\n 'F-Score': 0.8856447688564477}\n\n\nLos valores obtenidos en las métricas son muy buenos, con un accuracy, una sensibilidad y una especificidad de más de 0.9, podemos afirmar que el modelo en cuestión obtiene unos resultados muy buenos.\n\nmodelos.append(modelN3)"
  },
  {
    "objectID": "posts/Fases del Suenyo/index.html#modelo-rem",
    "href": "posts/Fases del Suenyo/index.html#modelo-rem",
    "title": "Detección de las Fases del Sueño mediante XGBoost.",
    "section": "5.6 Modelo REM",
    "text": "5.6 Modelo REM\n\n5.6.1 Creación del modelo\nPor último, realizamos el modelo para la fase REM (Rapid-eye movement), que como su nombre indica, tiene como característica principal el movimiento rápido de los ojos.\nLos parámetros obtenidos en el grid search son:\n\nlearning_rate = 0.4\n\n\nmax_depth = 5\n\n\nmin_child_weight = 1.5\n\n\nn_estimators = 1000\n\n\nreg_alpha = 0.5\n\n\nreg_lambda = 2\n\n\nmodelREM = xgb.XGBClassifier(learning_rate=0.4, \n                             max_depth=5, \n                             min_child_weight=1.5, \n                             n_estimators=1000, \n                             reg_alpha=0.5, \n                             reg_lambda=2)\n\"\"\"\nparam_grid = {\n    'n_estimators': [1000, 3000, 5000],\n    'max_depth': [4, 5, 6],\n    'learning_rate': [0.1, 0.3, 0.4],\n    'min_child_weight': [1.0, 1.5, 5],\n    'reg_alpha': [0, 0.1, 0.5],\n    'reg_lambda': [1, 2, 3]\n}\n\ngrid_search = GridSearchCV(\n    estimator=modelREM,\n    param_grid=param_grid,\n    scoring='accuracy',\n    cv=3,\n    verbose=1,\n    n_jobs=-1\n)\n\ngrid_search.fit(Xtrain,ytrain_bin[:,4])\n\nprint(\"Best Parameters:\", grid_search.best_params_)\nprint(\"Best Cross-Validation Accuracy:\", grid_search.best_score_)\n\nmodelREM = grid_search.best_estimator_\n\"\"\"\nmodelREM.fit(Xtrain,ytrain_bin[:,4])\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              feature_weights=None, gamma=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=0.4, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=5,\n              max_leaves=None, min_child_weight=1.5, missing=nan,\n              monotone_constraints=None, multi_strategy=None, n_estimators=1000,\n              n_jobs=None, num_parallel_tree=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBClassifier?Documentation for XGBClassifieriFitted\n        \n            \n                Parameters\n                \n\n\n\n\nobjective \n'binary:logistic'\n\n\n\nbase_score \nNone\n\n\n\nbooster \nNone\n\n\n\ncallbacks \nNone\n\n\n\ncolsample_bylevel \nNone\n\n\n\ncolsample_bynode \nNone\n\n\n\ncolsample_bytree \nNone\n\n\n\ndevice \nNone\n\n\n\nearly_stopping_rounds \nNone\n\n\n\nenable_categorical \nFalse\n\n\n\neval_metric \nNone\n\n\n\nfeature_types \nNone\n\n\n\nfeature_weights \nNone\n\n\n\ngamma \nNone\n\n\n\ngrow_policy \nNone\n\n\n\nimportance_type \nNone\n\n\n\ninteraction_constraints \nNone\n\n\n\nlearning_rate \n0.4\n\n\n\nmax_bin \nNone\n\n\n\nmax_cat_threshold \nNone\n\n\n\nmax_cat_to_onehot \nNone\n\n\n\nmax_delta_step \nNone\n\n\n\nmax_depth \n5\n\n\n\nmax_leaves \nNone\n\n\n\nmin_child_weight \n1.5\n\n\n\nmissing \nnan\n\n\n\nmonotone_constraints \nNone\n\n\n\nmulti_strategy \nNone\n\n\n\nn_estimators \n1000\n\n\n\nn_jobs \nNone\n\n\n\nnum_parallel_tree \nNone\n\n\n\nrandom_state \nNone\n\n\n\nreg_alpha \n0.5\n\n\n\nreg_lambda \n2\n\n\n\nsampling_method \nNone\n\n\n\nscale_pos_weight \nNone\n\n\n\nsubsample \nNone\n\n\n\ntree_method \nNone\n\n\n\nvalidate_parameters \nNone\n\n\n\nverbosity \nNone\n\n\n\n\n            \n        \n    \n\n\n\n\n5.6.2 Selección de características\nPrimero, utilizaremos los valores de Shapley (Shap) para obtener las variables más importantes y cómo afectan estas al modelo.\n\nexplainer = shap.TreeExplainer(modelREM)\nshap_values_REM = explainer.shap_values(Xtrain)\nshap_values_REM.shape\n\n(5130, 49)\n\n\n\nshap.summary_plot(shap_values_REM, Xtrain, show=False)\n\n\n\n\n\n\n\n\nObtenemos que eeg_abspow, emg_nzc y emg_iqr son las variables más importantes para el modelo REM.\nEstas variables afectan al modelo de la siguiente manera, interpretando el resultado obtenido:\n\neeg_abspow: Los valores más altos de esta variable afectan negativamente, es decir, si se tiene un valor de eeg_abspow alto, la predicción del modelo será que el paciente no está en la fase REM.\n\n\nemg_nzc: Al igual que en el caso anterior, y aparentemente de manera bastante más clara, los valores más altos significarán que el paciente no está en la fase REM, y los más bajos, que sí lo está.\n\n\nemg_iqr: Al igual que en las variables anteriores, pero de manera menos clara salvo para los valores muy elevados, los valores más altos de esta variable indicarán que el paciente no está en la fase REM.\n\nAhora mostramos las variables más importantes para el modelo y observamos si coinciden con el resultado obtenido en Shap.\n\nshow_features(modelREM)\n\n\n\n\n\n\n\n\nObtenemos que emg_nzc y eeg_abspow, al igual que en Shap, son variables muy importantes para el modelo. Sin embargo, la variable emg_iqr ahora no aparece entre las más importantes, ocupando su lugar eog_hcomp.\n\n\n5.6.3 Uso de Surrogate model para la interpretabilidad\n\nsurrogateREM_labels = modelREM.predict(Xtrain)\nsurrogateREM_model = DecisionTreeClassifier(random_state=42, max_depth=6)\nsurrogateREM_model.fit(Xtrain,surrogateREM_labels)\n\nplt.figure(figsize=(20,10))\nplot_tree(surrogateREM_model, filled=True, feature_names=Xtrain.columns, class_names=[\"No REM\",\"REM\"])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n5.6.4 Métricas\n\nmetricas_REM = show_metricas(\"REM\",yval_bin[:,4],modelREM.predict(Xval))\nmetricas.append(metricas_REM)\nmetricas_REM\n\n{'Modelo': 'REM',\n 'Accuracy': 0.9315849486887116,\n 'Precision': 0.9440559440559441,\n 'AP': 0.5798360416053232,\n 'AUC': 0.770625072200178,\n 'Sensibilidad': 0.5465587044534413,\n 'Especificidad': 0.9946914399469144,\n 'F-Score': 0.6923076923076923}\n\n\nObtenemos una especificidad excelente, así como grandes resultados en accuracy y precisión. Su principal debilidad es una sensibilidad bastante baja, por lo que se trata de un modelo bueno, pero no ideal.\n\nmodelos.append(modelREM)"
  },
  {
    "objectID": "posts/Fases del Suenyo/index.html#modelo-global",
    "href": "posts/Fases del Suenyo/index.html#modelo-global",
    "title": "Detección de las Fases del Sueño mediante XGBoost.",
    "section": "5.7 Modelo Global",
    "text": "5.7 Modelo Global\n\n5.7.1 Creación del modelo\n\ndef predict_model_XGBoost(modelos, X):\n    predicciones = modelos[0].predict(X)\n    for col in range(1,5):\n        if col == 1:\n            predicciones = np.column_stack((predicciones,modelos[col].predict_proba(X)[:,1]&gt;0.3))\n        else:\n            predicciones = np.column_stack((predicciones,modelos[col].predict(X)))\n    return predicciones\n\n\n\n5.7.2 Métricas\nAhora utilizaremos el conjunto de validación separado anteriormente para observar los resultados con un conjunto nuevo de datos antes de hacerlo con el test.\n\npred_global = predict_model_XGBoost(modelos,Xval)\nmetricas_global = show_metricas(\"Global\", yval_bin, pred_global, True)\nmetricas.append(metricas_global)\nmetricas_global\n\n{'Modelo': 'Global',\n 'Accuracy': 0.7451539338654504,\n 'Precision': 0.832278827063989,\n 'AP': 0.6156744985268198,\n 'AUC': None,\n 'Sensibilidad': 0.7782212086659065,\n 'Especificidad': None,\n 'F-Score': 0.7920827003924934}\n\n\nCreamos una matriz de confusión que nos ayuden a observar la proporción de los valores acertados, así como cuál ha sido la predicción cuando no ha acertado.\n\ndef procesar_filas(predicciones):\n    results = []\n    \n    for row in predicciones:\n        if np.sum(row) &gt;= 1:\n            results.append(np.argmax(row))  # Índice del valor más alto\n        else:\n            results.append(-2)\n            \n    return np.array(results)\n\nclases_pred = [0, 1, 2, 3, 4, -2]\nclases_label_pred = ['Wake', 'N1', 'N2', 'N3', 'REM', '-2']\nclases = ['Wake', 'N1', 'N2', 'N3', 'REM']\n\nyval_idx = np.argmax(yval_bin, axis=1)\npred_idx = procesar_filas(pred_global)\n\n# Crear una matriz de ceros\nmatriz_confusion = np.zeros((len(clases), len(clases_pred)))\n\nfor i in range(len(clases)):\n    for j in range(len(clases_pred)):\n        # Calcular la proporción entre clases\n        matriz_confusion[i, j] = np.sum((yval_idx == i) & (pred_idx == clases_pred[j])) / np.sum(yval_idx == i)\n\nconfusion_df = pd.DataFrame(matriz_confusion, index=clases, columns=clases_label_pred)\n\nprint(\"Tabla de proporciones:\")\nprint(confusion_df)\n\nTabla de proporciones:\n          Wake        N1        N2        N3       REM        -2\nWake  0.915663  0.018933  0.008606  0.000000  0.005164  0.051635\nN1    0.087209  0.319767  0.325581  0.000000  0.017442  0.250000\nN2    0.002865  0.037249  0.785100  0.103152  0.000000  0.071633\nN3    0.000000  0.000000  0.093827  0.871605  0.000000  0.034568\nREM   0.000000  0.072874  0.271255  0.004049  0.489879  0.161943\n\n\nEn general, obtenemos unos resultados bastante decentes, puesto que en general obtenemos resultados de 0.75-0.8 en la mayoría de las métricas, lo que indica que los modelos en general son bastante buenos y tienen altas capacidades. Asimismo, destaca como el 25% de los N1 no ha sido capaz de identificarlos y que el 35% lo ha identificado como N2. Por otro lado, la fase REM tampoco ofrece seguridad ya que el 24% lo ha asignado a N2 y el 21% no ha sido capaz de identificarlo. De este modo, se puede concluir que estas son las dos etapas más difíciles de predecir.\n\nmetricas = pd.DataFrame(metricas)\nmetricas\n\n\n\n\n\n\n\n\nModelo\nAccuracy\nPrecision\nAP\nAUC\nSensibilidad\nEspecificidad\nF-Score\n\n\n\n\n0\nWake\n0.962942\n0.970803\n0.916864\n0.951011\n0.915663\n0.986360\n0.942427\n\n\n1\nN1\n0.907640\n0.550000\n0.242577\n0.645661\n0.319767\n0.971555\n0.404412\n\n\n2\nN2\n0.860319\n0.614537\n0.531187\n0.837436\n0.799427\n0.875445\n0.694894\n\n\n3\nN3\n0.946408\n0.872902\n0.807909\n0.929739\n0.898765\n0.960712\n0.885645\n\n\n4\nREM\n0.931585\n0.944056\n0.579836\n0.770625\n0.546559\n0.994691\n0.692308\n\n\n5\nGlobal\n0.745154\n0.832279\n0.615674\nNaN\n0.778221\nNaN\n0.792083\n\n\n\n\n\n\n\nObservando todos los resultados juntos, vemos cómo el accuracy se ve penalizado al utilizar el conjunto de validación, pero el resto de variables dan aproximadamente el resultado esperado al observar su comportamiento previo.\nAhora creamos una nueva función que nos permite reproducir la curva ROC para cada uno de los modelos en base a sus predicciones.\n\ndef observe_results(pred, y):\n\n    for (label,col) in zip(sleep_stages.values(),np.arange(y.shape[1])):\n        fpr, tpr, thresholds = roc_curve(y[:,col], pred[:,col])\n        print(\"%s AUC: %f\" % (label,auc(fpr,tpr)))\n        plt.plot(fpr,tpr, label=label)\n    plt.legend()\n    plt.title(\"Curva ROC\")\n    plt.show()\n\nPrimero, observamos los resultados si utilizamos el conjunto de entrenamiento.\n\nobserve_results(predict_model_XGBoost(modelos, Xtrain), ytrain_bin)\n\nWake AUC: 1.000000\nN1 AUC: 1.000000\nN2 AUC: 1.000000\nN3 AUC: 1.000000\nREM AUC: 1.000000\n\n\n\n\n\n\n\n\n\nLa curva ROC que obtenemos es perfecta para todos los modelos, lo cual significa que puede predecir los datos del conjunto de entrenamiento a la perfección. Esto en general no se trata de un reflejo de las capacidades del modelo, puesto que estos son los datos con los que ha sido entrenado. A continuación se realizarán pruebas que serán más importantes a la hora de justificar la bondad de los modelos.\nAhora utilizamos el conjunto de validación.\n\nobserve_results(predict_model_XGBoost(modelos, Xval.values), yval_bin)\n\nWake AUC: 0.951011\nN1 AUC: 0.645661\nN2 AUC: 0.837436\nN3 AUC: 0.929739\nREM AUC: 0.770625\n\n\n\n\n\n\n\n\n\nAhora se observan mejor las capacidades reales de los modelos. Tenemos modelos como Wake y N3 con capacidades muy altas y que son capaces de predecir sus respectivas fases de manera satisfactoria. En cambio, otros modelos como REM o, principalmente, N1 no son capaces de captar sus fases tan bien, y pese a que no ofrecen resultados muy malos, puesto que son capaces de obtener bastante información correctamente, están bastante lejos de llegar a las capacidades de predicción de los modelos con mejores resultados."
  },
  {
    "objectID": "posts/Analisis Series Temporales/index.html",
    "href": "posts/Analisis Series Temporales/index.html",
    "title": "Análisis de Series Temporales",
    "section": "",
    "text": "Implementación de modelos Holt-Winters y ARIMA para el análisis de series temporales.\n\n\n\n\n Volver arriba"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Enrique Sayas Bailach",
    "section": "",
    "text": "Científico de Datos con formación en análisis estadístico, modelado predictivo y visualización de datos. Aplicación práctica de técnicas analíticas para la mejora de procesos, la toma de decisiones y la generación de valor. Conocimientos en resolución de problemas de optimización en contextos logísticos, control de versiones y metodologías de trabajo ágiles. Capacidad para transformar datos complejos en soluciones claras y útiles.\n\n\n Volver arriba"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Enrique Sayas Bailach",
    "section": "",
    "text": "Volver arriba"
  },
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "Mi Portfolio",
    "section": "",
    "text": "Trabajo Fin de Grado - Ciencia de Datos\n\n\n\nmascarillas\n\nciencia de datos\n\nanálisis\n\nR\n\n\n\n\n\n\n\n\n\n10 jul 2025\n\n\nEnrique Sayas Bailach\n\n\n\n\n\n\n\n\n\n\n\n\nMaximum Diversity Problem (MDP): Implementación de GRASP y Path-relinking.\n\n\n\nMDP\n\nciencia de datos\n\nPython\n\n\n\n\n\n\n\n\n\n16 may 2025\n\n\nCarles Vicent Adam Castañer, Luna Xinping Moreno Gómez, Enrique Sayas Bailach\n\n\n\n\n\n\n\n\n\n\n\n\nDetección de las Fases del Sueño mediante XGBoost.\n\n\n\nSalud\n\nciencia de datos\n\nXGBoost\n\nPython\n\n\n\n\n\n\n\n\n\n23 ene 2025\n\n\nCarlos Gila Blanco, Enrique Sayas Bailach\n\n\n\n\n\n\n\n\n\n\n\n\nEstación MedioAmbiental Inteligente\n\n\n\nIoT\n\nciencia de datos\n\nPYNQ Z2\n\nPython\n\n\n\n\n\n\n\n\n\n10 ene 2025\n\n\nCarlos Gila Blanco, Enrique Sayas Bailach\n\n\n\n\n\n\n\n\n\n\n\n\nAnálisis de Series Temporales\n\n\n\nseries temporales\n\nciencia de datos\n\nR\n\n\n\n\n\n\n\n\n\n22 dic 2023\n\n\nCarlos Gila Blanco, Enrique Sayas Bailach\n\n\n\n\n\n\n\n\n\n\n\n\n1er Premio en el Concurso de Visualización de Datos de la ciudad de Valencia 2023\n\n\n\nvalencia\n\nciencia de datos\n\ndatos abiertos\n\nR\n\n\n\n\n\n\n\n\n\n22 sept 2023\n\n\nJosep Antoni Girbés Plaza, Luna Xinping Moreno Gómez, Carles Vicent Adam Castañer, Carlos Gila Blanco, Enrique Sayas Bailach\n\n\n\n\n\n\n\n\n\n\n\n\nClasificación de canciones según su género en base a sus características\n\n\n\nclasificación\n\nciencia de datos\n\nmúsica\n\nR\n\n\n\n\n\n\n\n\n\n20 jun 2023\n\n\nCarlos Gila Blanco, Enrique Sayas Bailach\n\n\n\n\n\nNo hay resultados\n Volver arriba"
  },
  {
    "objectID": "posts/Clasificación Canciones/index.html",
    "href": "posts/Clasificación Canciones/index.html",
    "title": "Clasificación de canciones según su género en base a sus características",
    "section": "",
    "text": "Comparativa de diferentes modelos de clasificación haciendo uso del conjunto de datos Prediction of music genre.\n\n\n\n\n Volver arriba"
  },
  {
    "objectID": "posts/MDP/index.html",
    "href": "posts/MDP/index.html",
    "title": "Maximum Diversity Problem (MDP): Implementación de GRASP y Path-relinking.",
    "section": "",
    "text": "Implementación de GRASP y Path-relinking para la resolución del problema de Maximum Diversity Problem (MDP)\n\n\n\n\n Volver arriba"
  },
  {
    "objectID": "posts/Trabajo Fin de Grado - Ciencia de Datos/index.html",
    "href": "posts/Trabajo Fin de Grado - Ciencia de Datos/index.html",
    "title": "Trabajo Fin de Grado - Ciencia de Datos",
    "section": "",
    "text": "Trabajo Fin de Grado acerca del impacto del uso de mascarillas durante la pandemia del COVID-19 en distritos escolares del estado de Massachusetts.\n\n\n\n\n Volver arriba"
  }
]